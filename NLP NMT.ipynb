{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UhoLQDkGgdx8"
   },
   "outputs": [],
   "source": [
    "#Make imports\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q-_bF5bilI2f"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
    "  text = text.lower()\n",
    "  text = re.sub(r'\\d','',text)\n",
    "  text = re.sub(r'\\s+',' ',text)\n",
    "  text = text.strip()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration cfilt--iitb-english-hindi-911387c6837f8b91\n",
      "Found cached dataset parquet (C:/Users/joshi/.cache/huggingface/datasets/cfilt___parquet/cfilt--iitb-english-hindi-911387c6837f8b91/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3e30da43064c989a2c12eca69a82ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "corpus = load_dataset(\"cfilt/iitb-english-hindi\")\n",
    "data=corpus[\"validation\"][\"translation\"]+corpus[\"train\"][\"translation\"]+corpus[\"test\"][\"translation\"]\n",
    "Data=pd.DataFrame(data)\n",
    "hindi_sentences=Data[\"hi\"]\n",
    "english_sentences=Data[\"en\"]\n",
    "hindi_sentences = ['<START> ' + re.sub('[a-zA-Z]','',preprocess(hi)) + ' <END>' for hi in Data[\"hi\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gsf6EbkWZDUY",
    "outputId": "d90e379b-d6e3-4bd9-ed43-4626b8a7d0a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1662110 1662110\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0    Students of the Dattatreya city Municipal corp...\n",
       " 1    With encouragement from Principal Sandhya Medp...\n",
       " 2    Rajesh Gavre, the President of the MNPA teache...\n",
       " Name: en, dtype: object,\n",
       " ['<START> महानगर पालिका अंतर्गत दत्तात्रय नगर माध्यमिक स्कूल के विद्यार्थियों ने काल्पनिक किला दत्तगढ़ बनाकर अपनी कल्पनाशक्ति का परिचय दिया। <END>',\n",
       "  '<START> प्रधानाध्यापक संध्या मेडपल्लीवार के प्रोत्साहित करने पर शिक्षकों व विद्यार्थियों ने मिट्टïी से किले का निर्माण किया। <END>',\n",
       "  '<START> मनपा शिक्षक संघ के अध्यक्ष राजेश गवरे ने स्कूल को भेंट देकर सराहना की। <END>'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(english_sentences), len(hindi_sentences))\n",
    "print()\n",
    "english_sentences[:3], hindi_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EjnK8XArDPUe"
   },
   "outputs": [],
   "source": [
    "#Some parameters\n",
    "vocab_size = 10000\n",
    "total_sentences = 25000\n",
    "maxlen = 10\n",
    "epochs = 15\n",
    "validation_split = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KyHjYB_eMpD7"
   },
   "outputs": [],
   "source": [
    "en_data = []\n",
    "hi_data = []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for (en,hi) in zip(english_sentences, hindi_sentences):\n",
    "  l = min(len(en.split()), len(hi.split()))\n",
    "  if l <= maxlen:\n",
    "    en_data.append(en)\n",
    "    hi_data.append(hi)\n",
    "    cnt += 1\n",
    "  if cnt == total_sentences:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwR2rlOvB-xZ",
    "outputId": "f3c6d24a-d13b-46bd-ea0a-584f889bf7d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocab Size:  3641\n",
      "Hindi Vocab Size:  2382\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the texts and convert to sequences\n",
    "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
    "en_tokenizer.fit_on_texts(en_data)\n",
    "en_sequences = en_tokenizer.texts_to_sequences(en_data)\n",
    "\n",
    "hi_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
    "hi_tokenizer.fit_on_texts(hi_data)\n",
    "hi_sequences = hi_tokenizer.texts_to_sequences(hi_data)\n",
    "\n",
    "english_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "hindi_vocab_size = len(hi_tokenizer.word_index) + 1\n",
    "print(\"English Vocab Size: \", english_vocab_size)\n",
    "print(\"Hindi Vocab Size: \", hindi_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "luUa7TE6RYFq"
   },
   "outputs": [],
   "source": [
    "#Prepare encoder data\n",
    "encoder_inputs = tf.keras.preprocessing.sequence.pad_sequences(en_sequences, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "olIjin62TPF7"
   },
   "outputs": [],
   "source": [
    "#Prepare decoder data\n",
    "decoder_inputs = []\n",
    "decoder_outputs = []\n",
    "\n",
    "for hi in hi_sequences:\n",
    "  decoder_inputs.append(hi[:-1])\n",
    "  decoder_outputs.append(hi[1:])\n",
    "\n",
    "decoder_inputs = tf.keras.preprocessing.sequence.pad_sequences(decoder_inputs, maxlen=maxlen, padding='post')\n",
    "decoder_outputs = tf.keras.preprocessing.sequence.pad_sequences(decoder_outputs, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5OWbUrPRENr",
    "outputId": "44294d4c-4389-4e30-a210-5cfda4b3edac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23750, 10) (23750, 10) (23750, 10)\n"
     ]
    }
   ],
   "source": [
    "# Training and Testing split\n",
    "\n",
    "split = int(0.95 * total_sentences)\n",
    "\n",
    "X_train = [encoder_inputs[:split], decoder_inputs[:split]]\n",
    "y_train = decoder_outputs[:split]\n",
    "\n",
    "X_test = en_data[:split]\n",
    "y_test = hi_data[:split]\n",
    "\n",
    "print(X_train[0].shape, X_train[1].shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUboXx8WjDnP",
    "outputId": "1db42787-6c35-42cf-b9ca-b892bdc21935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, None, 256)    932096      ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, None, 256)    609792      ['input_16[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)                  [(None, 256),        525312      ['embedding_6[0][0]']            \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  [(None, None, 256),  525312      ['embedding_7[0][0]',            \n",
      "                                 (None, 256),                     'lstm_6[0][1]',                 \n",
      "                                 (None, 256)]                     'lstm_6[0][2]']                 \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, None, 2382)   612174      ['lstm_7[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,204,686\n",
      "Trainable params: 3,204,686\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define LSTM model\n",
    "d_model = 256\n",
    "\n",
    "#Encoder\n",
    "inputs = tf.keras.layers.Input(shape=(None,))\n",
    "x = tf.keras.layers.Embedding(english_vocab_size, d_model, mask_zero=True)(inputs)\n",
    "_,state_h,state_c = tf.keras.layers.LSTM(d_model,activation='relu',return_state=True)(x)\n",
    "\n",
    "#Decoder\n",
    "targets = tf.keras.layers.Input(shape=(None,))\n",
    "embedding_layer = tf.keras.layers.Embedding(hindi_vocab_size, d_model, mask_zero=True)\n",
    "x = embedding_layer(targets)\n",
    "decoder_lstm = tf.keras.layers.LSTM(d_model,activation='relu',return_sequences=True, return_state=True)\n",
    "x,_,_ = decoder_lstm(x, initial_state=[state_h, state_c])\n",
    "dense1 = tf.keras.layers.Dense(hindi_vocab_size, activation='softmax')\n",
    "x = dense1(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[inputs, targets],outputs=x)\n",
    "model.summary()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(optimizer='rmsprop', loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "nq3TUpTK3TRE"
   },
   "outputs": [],
   "source": [
    "#Save model after each epoch\n",
    "save_model_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='en-hi.h5',\n",
    "    monitor='val_accuracy',\n",
    "    mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "706/706 [==============================] - 128s 182ms/step - loss: 0.6562 - accuracy: 0.6925 - val_loss: 0.3396 - val_accuracy: 0.8053\n",
      "Epoch 2/10\n",
      "706/706 [==============================] - 122s 173ms/step - loss: 0.4702 - accuracy: 0.7837 - val_loss: 0.1894 - val_accuracy: 0.8924\n",
      "Epoch 3/10\n",
      "706/706 [==============================] - 139s 197ms/step - loss: 0.3362 - accuracy: 0.8456 - val_loss: 0.1043 - val_accuracy: 0.9356\n",
      "Epoch 4/10\n",
      "706/706 [==============================] - 136s 192ms/step - loss: 0.2424 - accuracy: 0.8897 - val_loss: 0.0514 - val_accuracy: 0.9715\n",
      "Epoch 5/10\n",
      "706/706 [==============================] - 123s 174ms/step - loss: 0.1855 - accuracy: 0.9190 - val_loss: 0.0319 - val_accuracy: 0.9853\n",
      "Epoch 6/10\n",
      "706/706 [==============================] - 125s 178ms/step - loss: 0.1575 - accuracy: 0.9325 - val_loss: 0.0185 - val_accuracy: 0.9911\n",
      "Epoch 7/10\n",
      "706/706 [==============================] - 130s 184ms/step - loss: 0.1402 - accuracy: 0.9389 - val_loss: 0.0131 - val_accuracy: 0.9944\n",
      "Epoch 8/10\n",
      "706/706 [==============================] - 122s 173ms/step - loss: 0.1278 - accuracy: 0.9437 - val_loss: 0.0132 - val_accuracy: 0.9930\n",
      "Epoch 9/10\n",
      "706/706 [==============================] - 109s 155ms/step - loss: 0.1201 - accuracy: 0.9462 - val_loss: 0.0125 - val_accuracy: 0.9937\n",
      "Epoch 10/10\n",
      "706/706 [==============================] - 122s 172ms/step - loss: 0.1128 - accuracy: 0.9489 - val_loss: 0.0121 - val_accuracy: 0.9944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x256a2c4adc0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, validation_split=validation_split, callbacks=[save_model_callback, tf.keras.callbacks.TerminateOnNaN()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gx0r37V9fshH",
    "outputId": "9b1f15a0-2675-41e1-cedf-9bbb550af9df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, None, 256)    932096      ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, None, 256)    609792      ['input_16[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)                  [(None, 256),        525312      ['embedding_6[0][0]']            \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  [(None, None, 256),  525312      ['embedding_7[0][0]',            \n",
      "                                 (None, 256),                     'lstm_6[0][1]',                 \n",
      "                                 (None, 256)]                     'lstm_6[0][2]']                 \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, None, 2382)   612174      ['lstm_7[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,204,686\n",
      "Trainable params: 3,204,686\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Retrieve previously saved stuff\n",
    "saved_model = tf.keras.models.load_model('en-hi.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = saved_model.get_layer('input_11').output\n",
    "_,state_h,state_c = saved_model.get_layer('lstm_4').output\n",
    "targets = saved_model.get_layer('input_12').output\n",
    "embedding_layer = saved_model.get_layer('embedding_5')\n",
    "decoder_lstm = saved_model.get_layer('lstm_5')\n",
    "dense1 = saved_model.get_layer('dense_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "FBvZo1SRzHbU"
   },
   "outputs": [],
   "source": [
    "#Inference Model\n",
    "\n",
    "#Encoder\n",
    "encoder = tf.keras.models.Model(inputs, [state_h, state_c])\n",
    "\n",
    "#Decoder\n",
    "decoder_input_h = tf.keras.layers.Input(shape=(d_model,))\n",
    "decoder_input_c = tf.keras.layers.Input(shape=(d_model,))\n",
    "x = embedding_layer(targets)\n",
    "x, decoder_output_h, decoder_output_c = decoder_lstm(x, initial_state=[decoder_input_h, decoder_input_c])\n",
    "x = dense1(x)\n",
    "decoder = tf.keras.models.Model([targets] + [decoder_input_h, decoder_input_c], \n",
    "                                [x] + [decoder_output_h, decoder_output_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "DM4WlWIkXbS4"
   },
   "outputs": [],
   "source": [
    "def predict_sentence(en_input):\n",
    "  input_seq = en_tokenizer.texts_to_sequences([en_input])\n",
    "\n",
    "  next_h, next_c = encoder.predict(input_seq)\n",
    "\n",
    "  curr_token = np.zeros(1)\n",
    "  curr_token[0] = hi_tokenizer.word_index['<START>']\n",
    "\n",
    "  pred_sentence = ''\n",
    "\n",
    "  for i in range(maxlen):\n",
    "    output, next_h, next_c = decoder.predict([curr_token] + [next_h , next_c])\n",
    "    next_token = np.argmax(output[0, 0, :])\n",
    "    next_word = hi_tokenizer.index_word[next_token]\n",
    "    if next_word == '<END>':\n",
    "      break\n",
    "    else:\n",
    "      pred_sentence += ' ' + next_word\n",
    "      curr_token[0] = next_token\n",
    "\n",
    "  return pred_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ramesh Saatpute examined the fort.\n",
      "रमेश सातपुते ने किले का निरीक्षण किया\n"
     ]
    }
   ],
   "source": [
    "l = len(X_test[0].split())\n",
    "print(X_test[0])\n",
    "pred_sentence = predict_sentence(X_test[0])\n",
    "print(pred_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score is : 40.72\n"
     ]
    }
   ],
   "source": [
    "#Testing and Analysis\n",
    "import nltk\n",
    "\n",
    "candidates = []\n",
    "references = []\n",
    "\n",
    "import googletrans \n",
    "translator = googletrans.Translator()\n",
    "\n",
    "ctr = 20 \n",
    "i = 0\n",
    "\n",
    "while ctr>0:\n",
    "  l = len(X_test[i].split())\n",
    "  if l<=maxlen:\n",
    "    pred_sentence = predict_sentence(X_test[i])\n",
    "    candidates.append(pred_sentence.split())\n",
    "    google_translated_sentence = translator.translate(X_test[i], dest='hi').text\n",
    "    print()\n",
    "    references.append([y_test[i].split()[1:-1], google_translated_sentence.split()])\n",
    "\n",
    "    ctr -= 1\n",
    "  i += 1\n",
    "\n",
    "print(\"BLEU Score is : \",nltk.translate.bleu_score.corpus_bleu(references, candidates))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyPTo1CrpPJJs+er6sXjlNmJ",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1HelTzK5x_6q8uk0qmxzP533Di5H1hAWC",
   "name": "Neural Machine Translation using LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
